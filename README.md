### 一、简单介绍：
CIFAR10数据集是由十类32\*32*3的彩色图像组成的，如飞机、鸟等类别下各有若干图片，现采用卷积神经网络对十类图片做分类任务，并调节参数分析作用。
### 二、模型选择：
采用两个卷积层两个池化层三个全连接层实现。可以调节的参数有：卷积层1的输出通道数、卷积核大小、步长（选择默认），卷积层2的输出通道数、卷积核大小、步长（选择默认），池化层大小（选择2不改变）、池化方式（选择MaxPool2d），全连接层神经元个数，学习轮次，学习率，学习动量大小，批大小。
### 三、初始参数设置及效果：
self.conv1 = nn.Conv2d(3, 6, 5)  
self.pool = nn.MaxPool2d(2, 2)  
self.conv2 = nn.Conv2d(6,12 , 5)  
self.fc1 = nn.Linear(12 * 5 * 5, 120)  
self.fc2 = nn.Linear(120, 84)  
self.fc3 = nn.Linear(84, 10)  
#学习轮次2 学习率0.001 动量0.9 批次大小32  

效果如下： ![参数调整前效果](https://github.com/ZhouZhongZeWHU/CIFAR10/blob/main/beforeResult.png)
可见，准确率不高，经过每两百个图片的学习后损失下降也很慢
### 四、参数调整
1.	学习率和学习轮次调整：初始学习率过小，导致损失函数收敛很慢，而训练轮次过少导致网络还未到达最优解就停止了学习。考虑增大学习率到0.01（不能过大以免越过最低点），学习轮次提高到5（过高会导致电脑负荷增大，也有过拟合风险）。结果：损失在每一轮次间下降很快，轮次中会上下波动，最后准确率达到61%，损失达到0.110。
2.	批大小和动量调整：当 momentum 动量越大时，其转换为势能的能量也就越大，就越有可能摆脱局部凹域的束缚，进入全局凹域，将动量增大到0.92，批大小根据经验一般是数据量开根号，这里调整到50。结果：与上一次调整区别不大。
3.	卷积层通道数调整：通道数越多可能可以提取出更加精细的信息，卷积层1输出通道数调整为9，卷积层2输出通道数调整为18。结果： 最后一轮训练中损失在0.105波动，准确率略微提升至63%。
4.	卷积核大小调整：尝试调整第一层卷积核大小为9，第二层不变（若第一、二层卷积核大小改变为7、9则输入到全连接层的尺寸会过小）。结果：准确率下降到55%，卷积核过大，恢复为初始卷积核大小。
5.	全连接层神经元个数调整：现考虑输入输出层共有四层，输入18\*5*5，输出10，现遵从习惯调整为450,256,64,10，最后结果为： ![参数调整后效果](https://github.com/ZhouZhongZeWHU/CIFAR10/blob/main/afterResult.png)相比初始参数，效果提升了百分之百以上。

#### 代码见main.py
